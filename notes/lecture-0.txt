Lecture 0

- Search
    
    "We would like the AI to be able to search for solutions"
    
    Example: Driving directions (google mnaps)
    
    Terms:
        Agent: Entity that perceives its environment and acts upon that environment
        State: a configuration of the agent in its environment
            Initial State: the state where the agent begins
        Actions: choices that can be made in a state
            - as a function:
                Actions(s)
                    returns the set of actions that can be executed in state s 
        Transition Model: a description of what state results from performing any action in any state
            - as a function
                Result(s,a)
                    returns the state resulting from performing action a in state s
        State Space: the set of all states reachable from the initial state by any sequence of actions
        Goal Test: way to determine wheter a given state is a goal state
        Path Cost: numerical cost associated with a given path
        Solution: a sequence of actions that leads from the initial state to the goal state
            Optimal Solution: a solution that has the lowest path cost among all solutions
        node: a data structure that keeps track of 
            - a state
            - a parent (node that generated this node)
            - an action (action applied to parent to get node)
            - a path cost (from initial state to node)
        stack: last-in first-out data type
        queue: first-in first-out data type
        depth-first search (DFS): search algorithm that always expands the deepest node in the frontier
            - ex: goes down route until it hits dead end
            - won't always find most optimal path
        breadth-first search (BFS): search algorithm that always expands the shallowest node in the frontier
            - ex: looks at nodes that are 1 node away from initial state, then 2, then 3 etc.
            - BFS will sometimes, but not always, find a shorter path than DFS
        greedy best-first search (GBFS): search algorithm that expands the node that is closest to the goal, as estimated by a heuristic function h(n)
            - h(n) = estimated cost to goal
        A* search: search algorithm that expands node with lowest value of g(n) + h(n)
            - g(n) = cost to reach node
            - h(n) = estimated cost to goal
            - ex: how many steps did I take to get where I am? And how far is this from the goal?
            - optimal if:
                - h(n) is admissible (never overestimates the true cost)
                - h(n) is consistent (every node n and successor n' with step cost c)(h(n) <= h(n') + c)
            - the heuristic value from me to the goal shouldn't be more than the heuristic value of my successor
        uninformed search: search strategy that uses no problem-specific knowledge (DFS + BFS)
        informed search: search strategy that uses problem-specific knowledge to find solutions more efficiently (greedy best-first search + A* search)

    Search Problem:
        - initial state
        - actions
        - transition model
        - goal test
        - path cost function

    Approach Examples
        - start with a frontier that contains the initial state
        - repeat:
            - if frontier is empty, then no solution 
            - remove a node from the frontier
            - if node contains goal state, return the solution
            - expand node, add resulting nodes to the frontier
        - pitfall
            - can get caught in infinite loop of nodes have more than one direction
                - ex: A --> B but B can go B --> A, B --> C, B --> D
    
    Revised Approach
        - start with a frontier that contains the initial state
        - start with an empty explored set
        - repeat:
            - if frontier is empty, then no solution 
            - remove a node from the frontier
            - if node contains goal state, return the solution
            - add the node to the explored set
            - expand node, add resulting nodes to the frontier if they are not already in the frontier or the explored set
        This is called "Depth-First Search"
    
    Choosing the heuristic can be the challenge

    Adversarial Search
        - one agent trying to get to a goal while another is trying to stop it
        - ex: tic-tac-toe
    
        Minimax Algorithm
            - The computer doesn't normally understand notions of win or lose, but it does understand the concept of bigger and smaller.
            - assign value to goal states
                - O win = -1
                - nobody win = 0
                - X win = 1
                - X is "max player", O is "min player"
            - Max aims to maximize score
            - Min aims to minimize score

        How we build a tic-tac-toe game
            Game
                - S0: initial state
                - Player(s): returns which player to move in state s
                - Actions(s): returns legal moves in state S
                - Result(s, a): returns state after action a taken in state s
                - Terminal(s): checks if state s is a terminal state
                    - state is some Player result is a 3 in a row or
                    - all the squares of the board are filled
                - Minimax
                    - given a state (s):
                         - MAX picks an action (a) in Action(s) that produces highest value of Min-Value(Result(s, a))
                         - MIN picks an action (a) in Action(s) that produces smallest value of Max-Value(Result(s, a))
                    psudo code 

                        function MAX-VALUE(state):
                            if TERMINAL(state):
                                return UTILITY(state)
                        v = -∞  (We want the value initially to be as low as possible, because as I consider my actions, I'm always going to try and do better than v. And if I set v to negative infinity, I know I can always do better than that.)
                        for action in ACTIONS(state):
                            v = MAX(v, MIN-VALUE(RESULT(state, action)))
                            return v
                        
                        function MIN-VALUE(state):
                            if TERMINAL(state):
                                return UTILITY(state)
                        v = ∞  (We set it to infinity, because I know it can always get something less than infinity. So by starting with v equals infinity, I make sure that the very first action we find--that will be less than this value of v.)
                        for action in ACTIONS(state):
                            v = MIN(v, MAX-VALUE(RESULT(state, action)))
                            return v
            
            Optimizations
                - Alpha-Beta Pruning: Seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree
                - Depth-limited Minimax: A variant of the Minimax algorithm that explores a game tree only up to a specified depth, rather than to its full extent
                - Evaluation Function: A function that estimates the expected utility of the game from a given state